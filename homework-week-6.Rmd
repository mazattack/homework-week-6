---
title: "homework-week-6"
author: "Maria C. Codlin"
date: "October 17, 2017"
output: html_document
---
[1] Write a simple R function, Z.prop.test(), that can perform one- or two-sample Z-tests for proportion data, using the following guidelines.
Your function should take the following arguments: p1 and n1 (no default) representing the estimated proportion and sample size (i.e., based on your sample data); p2 and n2 (both defaulting to NULL) that contain a second sample’s proportion and sample size data in the event of a two-sample test; p0 (no default) as the expected value for the population proportion; and alternative (default “two.sided”) and conf.level (default 0.95), to be used in the same way as in the function t.test().

When conducting a two-sample test, it should be *p1 that is tested as being smaller or larger than p2 when alternative=“less” or alternative=“greater”*, the same as in the use of x and y in the function t.test().

The function should perform a one-sample Z-test using p1, n1, and p0 if either p2 or n2 (or both) is NULL.

The function should contain a check for the rules of thumb we have talked about (n∗p>5n∗p>5 and n∗(1−p)>5n∗(1−p)>5) to ensure the validity of assuming the normal distribution in both the one- and two-sample settings.

If this is violated, the function should still complete but it should also print an appropriate warning message.

The function should return a list containing: 
    the members Z (the test statistic), 
    P (the appropriate p value), and 
    CI (the two-sided CI with respect to “conf.level” around p1 in the case of a one-sample test and around p2-p1 in the case of a two-sample test). 
    For all test alternatives (“two.sided”, “greater”, “less”), calculate symmetric CIs based on quantiles of the normal distribution rather than worrying about calculating single-limit confidence bounds.


```{r function part 1, one sample prop test}
Z.prop.test<-function(p1, n1, p2=NULL, n2=NULL, p0, alternative="two.sided", conf.level=0.95) {
  
    if(is.null(p2)||is.null(n2)) {
      
  #one sample test      
if ((n1∗p1<=5) | n1∗(1−p1)<=5) warning("Warning: failed one or more validity tests, data may not be normally distributed") #in the module, this is given here refering to pi, not p-hat, but Davies (2016) writes this with p-hat.      
    if (alternative == "two.sided") {
      z <- (p1 - p0)/sqrt(p0 * (1 - p0)/n1)
      p <- 1 - pnorm(z, lower.tail = TRUE) + pnorm(z, lower.tail = FALSE)
    lower <- p1 - qnorm((1 - (1 - conf.level)/2)) * sqrt(p1 * (1 - p1)/n1)
    upper <- p1 + qnorm((1 - (1 - conf.level)/2)) * sqrt(p1 * (1 - p1)/n1)
    ci <- c(lower, upper)
    }
  
    if (alternative =="greater") {
      if (p1<p0) stop ("p0>p1, use alternative='less'")
    z <- (p1 - p0)/sqrt(p0 * (1 - p0)/n1)
    p <- pnorm(z, lower.tail=FALSE)
    lower <- p1 - qnorm((1 - (1 - conf.level)/2)) * sqrt(p1 * (1 - p1)/n1)
    upper <- p1 + qnorm((1 - (1 - conf.level)/2)) * sqrt(p1 * (1 - p1)/n1)
    ci <- c(lower, upper)
    }
    
    if (alternative =="less") {
      if (p1>p0) stop ("p1>p0, use alternative='greater'")
    z <- (p1-p0)/sqrt(p0 * (1 - p0)/n1)
    p <- pnorm(z, lower.tail=TRUE)
    lower <- p1 - qnorm((1 - (1 - conf.level)/2)) * sqrt(p1 * (1 - p1)/n1)
    upper <- p1 + qnorm((1 - (1 - conf.level)/2)) * sqrt(p1 * (1 - p1)/n1)
    ci <- c(lower, upper)
    }
    }
  else{
  #two sample test
    
    if ((n1∗p1<=5) | n1∗(1−p1)<=5 | (n2∗p2<=5) | n2∗(1−p2)<=5) warning ("Warning: failed one or more validity tests, data may not be normally distributed") #in the module, this is given here refering to pi, not p-hat, but Davies (2016) writes this with p-hat.
    
    if (alternative == "two.sided") {
  pstar<-(p1 * n1 + p2 * n2) / (n1 + n2)
  z <- (p2 - p1)/sqrt((pstar * (1 - pstar)) * (1/n1 + 1/n2))
  p <- 1 - pnorm(z, lower.tail = TRUE) + pnorm(z, lower.tail = FALSE)
  lower <- (p2 - p1) - qnorm((1 - (1 - conf.level)/2)) * sqrt((p1*(1-p1))/n1 + (p2*(1 - p2))/(n2))
  upper <- (p2 - p1) + qnorm((1 - (1 - conf.level)/2)) * sqrt((p1*(1-p1))/n1 + (p2*(1 - p2))/(n2))
  ci <- c(lower, upper)
    }
  
    if (alternative =="greater") {
      if (p1>p2) stop ("p2>p1, use alternative='less'")
   
  pstar<-(p1 * n1 + p2 * n2) / (n1 + n2)
  z <- (p2 - p1)/sqrt((pstar * (1 - pstar)) * (1/n1 + 1/n2))
  p <- 1 - pnorm(z, lower.tail = FALSE)
  lower <- (p2 - p1) - qnorm((1 - (1 - conf.level)/2)) * sqrt((p1*(1-p1))/n1 + (p2*(1 - p2))/(n2))
  upper <- (p2 - p1) + qnorm((1 - (1 - conf.level)/2)) * sqrt((p1*(1-p1))/n1 + (p2*(1 - p2))/(n2))
  ci <- c(lower, upper)
      
    }
    
  if (alternative =="less") {
    if (p1<p2) stop ("p1>p2, use alternative='greater'")
      pstar<-(p1 * n1 + p2 * n2) / (n1 + n2)
      z <- (p2 - p1)/sqrt((pstar * (1 - pstar)) * (1/n1 + 1/n2))
      p <- 1 - pnorm(z, lower.tail = TRUE)
      lower <- pstar - qnorm((1 - (1 - conf.level)/2)) * sqrt(pstar * (1 - pstar)/(n1+n2))
      upper <- pstar + qnorm((1 - (1 - conf.level)/2)) * sqrt((p1*(1-p1))/n1 + (p2*(1 - p2))/(n2))
      ci <- c(lower, upper)
    }}
  
    l<-list(
    "Z" = z,
    "p" = p,
    "Confidence interval" = ci)
  return (l) 
    }


```

```{r}
Z.prop.test(p1=0.56, p2=0.7, n1=25, n2=30, alternative="two.sided")
Z.prop.test(p1=0.56, p2=0.7, n1=25, n2=30, alternative="greater")
Z.prop.test(p1=0.56, p2=1, n1=25, n2=30, alternative="greater")
```
```{r}
Z.prop.test(p1=0.6, p0=0.8, n1=30)
Z.prop.test(p1=0.9, p0=0.8, n1=10, alternative = "greater")
```

#PART 2

[2] The dataset from Kamilar and Cooper has in it a large number of variables related to life history and body size. For this exercise, the end aim is to fit a simple linear regression model to predict longevity (MaxLongevity_m) measured in months from species’ brain size (Brain_Size_Species_Mean) measured in grams. Do the following for both  longevity~brain size and  log(longevity)~log(brain size).

Fit the regression model and, using {ggplot2}, produce a scatterplot with the fitted line superimposed upon the data. Append the the fitted model equation to your plot (HINT: use the function geom_text()).


Identify and interpret the point estimate of the slope (β1β1), as well as the outcome of the test associated with the hypotheses H0: β1β1 = 0; HA: β1β1 ≠ 0. Also, find a 90 percent CI for the slope (β1β1) parameter.

Using your model, add lines for the 90 percent confidence and prediction interval bands on the plot and add a legend to differentiate between the lines.

Produce a point estimate and associated 90 percent PI for the longevity of a species whose brain weight is 800 gm. Do you trust the model to predict observations accurately for this value of the explanatory variable? Why or why not?
Looking at your two models, which do you think is better? Why?
```{r}
library(curl)
f <- curl("https://raw.githubusercontent.com/fuzzyatelin/fuzzyatelin.github.io/master/AN597_Fall17/KamilarAndCooperData.csv")
d <- read.csv(f, header = TRUE, stringsAsFactors = FALSE)
head(d)
```

##Fit the regression model and, using {ggplot2}, produce a scatterplot with the fitted line superimposed upon the data. Append the the fitted model equation to your plot (HINT: use the function geom_text()).
```{r}
x1<-d$Brain_Size_Species_Mean
y1<-d$MaxLongevity_m
x2<-log(x1)
y2<-log(y1)
```
```{r}
m <- lm(y1 ~ x1, data = d)
m
m2 <- lm(y2 ~ x2, data = d)
m2
```
```{r}
rout <- list(paste('Fitted model: ', round(coef(m)[1], 3), ' + ',
                               round(coef(m)[2], 3), ' x', sep = ''),
              paste('R^2 == ', round(summary(m)[['r.squared']], 3),
sep = '')  )

```

```{r}
rout2 <- list(paste('Fitted model: ', round(coef(m2)[1], 3), ' + ',
                               round(coef(m2)[2], 3), ' x', sep = ''),
              paste('R^2 == ', round(summary(m2)[['r.squared']], 3),
sep = '')  )
```


```{r}
library(ggplot2)
g <- ggplot(data = d, aes(x = Brain_Size_Species_Mean, y = MaxLongevity_m))
g <- g + geom_point()
g <- g + geom_smooth(method = "lm", formula = y ~ x)
g <- g + geom_text(aes(x = 2, y = 50, label = rout[[1]]), hjust = 0) +
   geom_text(aes(x = 2, y = 0, label = rout[[2]]), hjust = 0, parse = TRUE)
g
```
**##Identify and interpret the point estimate of the slope (β1), as well as the outcome of the test associated with the hypotheses H0: β1 = 0; HA: β1 ≠ 0. Also, find a 90 percent CI for the slope (β1) parameter.**

The slope β1 is 1.218 which suggests that for every increase of 1 gram of brain weight, longevity of the species is predicted to increase by 1.218 months. This demonstrates there is a positive relationship between these two variables and supports the alternative hypothesis that β1 does not equal 0 (p<0.0001). Based on the R squared value, the variation in brainsize describes roughly 49% of the variation in longevity.
```{r}
summary(m)
```
```{r}
ci <- confint(m, level = 0.90)  # using the results of lm()
ci
```
The 90% confidence interval for β1 suggests there is a 90 percent chance the true mean of the slope for brainweight and longevity is between is 1.04-1.4.


```{r}
library(ggplot2)
g <- ggplot(data = d, aes(x = log(Brain_Size_Species_Mean), y = log(MaxLongevity_m)))
g <- g + geom_point()
g <- g + geom_smooth(method = "lm", formula = y ~ x)
g <- g + geom_text(aes(x = 1, y = 4.4, label = rout2[[1]]), hjust = 0) +
   geom_text(aes(x = 1, y = 4.25, label = rout2[[2]]), hjust = 0, parse = TRUE)
g
```
**##Identify and interpret the point estimate of the slope (β1), as well as the outcome of the test associated with the hypotheses H0: β1 = 0; HA: β1 ≠ 0. Also, find a 90 percent CI for the slope (β1) parameter.**

In the log regression model, the slope is 0.234, which suggests for every 1 increase of log brain size, log lengivity increases by 0.234. This relationship with log brainsize explains about 58 % of the variation in loglongevity. This too is a positive relationship between the two variables which is significant (p<0.0001).The 90% confidence interval is 0.205 - 0.264, which means it is 90% likely that the true mean of the slope is between these two values. 

```{r}
summary(m2)
```

```{r}
ci <- confint(m2, level = 0.90)  # using the results of lm()
ci
```

**Using your model, add lines for the 90 percent confidence and prediction interval bands on the plot and add a legend to differentiate between the lines.**


```{r}
v <- seq(from = 0, to = 500, by = 5)
m<- lm(data = d, MaxLongevity_m ~ Brain_Size_Species_Mean)
ci <- predict(m, newdata = data.frame(Brain_Size_Species_Mean = v), interval = "confidence", level = 0.90)
pi <- predict(m, newdata = data.frame(Brain_Size_Species_Mean = v), interval = "prediction", level = 0.90)
plot(data = d, MaxLongevity_m~ Brain_Size_Species_Mean)
lines(x = v, y = ci[, 1], col = "black")
lines(x = v, y = ci[, 2], col = "blue")
lines(x = v, y = ci[, 3], col = "blue")
lines(x = v, y = pi[, 2], col = "red")
lines(x = v, y = pi[, 3], col = "red")
legend("bottomright", legend = c("90% confidence interval of sample slope", "expected range of longevity given brainsize (90% CI)"),  col=c("blue","red"), lty=c(1,1))
```

**Produce a point estimate and associated 90 percent PI for the longevity of a species whose brain weight is 800 gm. Do you trust the model to predict observations accurately for this value of the explanatory variable? Why or why not? Looking at your two models, which do you think is better? Why?**

```{r}
ci <- predict(m, newdata = data.frame(Brain_Size_Species_Mean = 800), interval = "confidence", 
    level = 0.90)
ci
```
The model predicts that a species with average brainweight of 800gm will have a longevity of 1223 months, and suggests that 90% of the time, the average longevity of a species with 800gm brain weight will fall between 1089 and 1357 months. 


**Using your model, add lines for the 90 percent confidence and prediction interval bands on the plot and add a legend to differentiate between the lines.**
```{r}
v <- seq(from = 0, to = 7, by = .25)
logs<-cbind.data.frame("logbrainsize"=log(d$Brain_Size_Species_Mean), "loglongevity"=log(d$MaxLongevity_m))
m2<- lm(data = logs, loglongevity~logbrainsize)
ci <- predict(m2, newdata = data.frame(logbrainsize = v), interval = "confidence", level = 0.90)
pi <- predict(m2, newdata = data.frame(logbrainsize = v), interval = "prediction", level = 0.90)
plot(data = logs, loglongevity~ logbrainsize)
lines(x = v, y = ci[, 1], col = "black")
lines(x = v, y = ci[, 2], col = "blue")
lines(x = v, y = ci[, 3], col = "blue")
lines(x = v, y = pi[, 2], col = "red")
lines(x = v, y = pi[, 3], col = "red")
legend("bottomright", legend = c("90% confidence interval of sample slope", "expected range of longevity given brainsize (90% CI)"),  col=c("blue","red"), lty=c(1,1))
```

**Produce a point estimate and associated 90 percent PI for the longevity of a species whose brain weight is 800 gm. Do you trust the model to predict observations accurately for this value of the explanatory variable? Why or why not? Looking at your two models, which do you think is better? Why?**

```{r}
ci <- predict(m2, newdata = data.frame(logbrainsize = log(800)), interval = "confidence", level = 0.90)
ci
exp(ci)
```
The model predicts that a species with average brain weight of 800gm will have a longevity of 629 months, and suggests that 90% of the time, the average longevity of a species with 800gm brain weight will fall between 571 and 692 months. This model is likely more accurate, as the relationship describe in the log values accounts for more of the variation, suggesting that perhaps the data were not normally distributed and/or the relationship not linear. 

